{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing minAction.net Test Results\n",
    "\n",
    "This notebook walks through reproducing the empirical validation results reported in the paper.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.model_interface import OllamaInterface\n",
    "from src.evaluation import evaluate_test\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Existing Results\n",
    "\n",
    "First, let's load and examine the results from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detailed results\n",
    "with open('../results/qwen2_math_7b/detailed_results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Display metadata\n",
    "print(\"Metadata:\")\n",
    "for key, value in results['metadata'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Results by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test data\n",
    "tests = results['tests']\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'Test': t['name'],\n",
    "        'Category': t['category'],\n",
    "        'Score': t['score'],\n",
    "        'Status': t['status']\n",
    "    }\n",
    "    for t in tests\n",
    "])\n",
    "\n",
    "# Plot by category\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "category_scores = df.groupby('Category')['Score'].mean()\n",
    "category_scores.plot(kind='bar', ax=ax, color='steelblue')\n",
    "ax.set_ylabel('Average Score')\n",
    "ax.set_title('Performance by Test Category')\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.axhline(y=0.61, color='r', linestyle='--', label='Overall Average (61%)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCategory Breakdown:\")\n",
    "print(category_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reproduce a Single Test\n",
    "\n",
    "Let's reproduce Test 1 (General Lagrangian) to verify our setup works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model interface\n",
    "model = OllamaInterface(model_name='qwen2-math:7b')\n",
    "\n",
    "# Load prompt\n",
    "with open('../prompts/phase1_basic/test_1.txt', 'r') as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Get response\n",
    "response = model.generate(prompt)\n",
    "print(\"Model Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Manual Evaluation\n",
    "\n",
    "Compare this response to the one in detailed_results.json. Does it match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original response from paper\n",
    "test_1_original = [t for t in tests if t['test_id'] == 1][0]\n",
    "print(\"Original Response from Paper:\")\n",
    "print(test_1_original['model_response'])\n",
    "print(\"\\nScore:\", test_1_original['score'])\n",
    "print(\"Status:\", test_1_original['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Full Test Suite (Optional)\n",
    "\n",
    "**Warning**: This will make 9 API calls and take ~2-3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run full test suite\n",
    "# !python ../scripts/run_tests.py --model qwen2-math:7b --output ../results/reproduction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Phase 1 vs Phase 2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 results\n",
    "phase2_tests = results.get('phase2_tests', [])\n",
    "if phase2_tests:\n",
    "    df_phase2 = pd.DataFrame([\n",
    "        {\n",
    "            'Test': t['name'],\n",
    "            'Phase 1 Score': t.get('original_score', 0),\n",
    "            'Phase 2 Score': t['score'],\n",
    "            'Improvement': t.get('improvement', 'N/A')\n",
    "        }\n",
    "        for t in phase2_tests\n",
    "    ])\n",
    "    \n",
    "    print(\"Phase 2 (With Selection Principles) Results:\")\n",
    "    print(df_phase2)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = range(len(df_phase2))\n",
    "    width = 0.35\n",
    "    ax.bar([i - width/2 for i in x], df_phase2['Phase 1 Score'], width, label='Phase 1 (Basic)', alpha=0.8)\n",
    "    ax.bar([i + width/2 for i in x], df_phase2['Phase 2 Score'], width, label='Phase 2 (Guided)', alpha=0.8)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Phase 1 vs Phase 2 Performance')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_phase2['Test'], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Key findings from the empirical validation:\n",
    "\n",
    "1. **Mathematical Capability Without Physical Intuition**: The model achieves 61% on forward derivation but 0% on inverse problems\n",
    "2. **Selection Principles Help**: Providing explicit principles improves performance by 14 percentage points\n",
    "3. **Inverse Problems Remain Unsolved**: Even with guidance, construction tasks fail completely\n",
    "\n",
    "These results validate the need for the minAction.net framework - current architectures lack the right inductive bias for physics discovery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
